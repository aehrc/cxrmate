{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import tokenizers\n",
    "import transformers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Paths:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dataset_dir = 'B:/source/Datasets'\n",
    "splits_path = os.path.join(\n",
    "    dataset_dir,\n",
    "    'mimic_cxr_jpg',\n",
    "    'physionet.org',\n",
    "    'files',\n",
    "    'mimic-cxr-jpg',\n",
    "    '2.0.0',\n",
    "    'mimic-cxr-2.0.0-split.csv',\n",
    ")\n",
    "reports_path = os.path.join(dataset_dir, 'mimic_cxr_sections', 'mimic_cxr_sectioned.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training corpus:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                       dicom_id  study_id  subject_id  split  \\\n0  02aa804e-bde0afdd-112c0b34-7bc16630-4e384014  50414267    10000032  train   \n1  174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962  50414267    10000032  train   \n2  2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab  53189527    10000032  train   \n3  e084de3b-be89b11e-20fe3f9f-9c8d8dfe-4cfd202c  53189527    10000032  train   \n4  68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714  53911762    10000032  train   \n\n                                          impression  \\\n0  There is no focal consolidation, pleural effus...   \n1  There is no focal consolidation, pleural effus...   \n2  The cardiac, mediastinal and hilar contours ar...   \n3  The cardiac, mediastinal and hilar contours ar...   \n4  Single frontal view of the chest provided. The...   \n\n                                            findings last_paragraph  \\\n0  There is no focal consolidation, pleural effus...            NaN   \n1  There is no focal consolidation, pleural effus...            NaN   \n2  The cardiac, mediastinal and hilar contours ar...            NaN   \n3  The cardiac, mediastinal and hilar contours ar...            NaN   \n4  Single frontal view of the chest provided. The...            NaN   \n\n             comparison  \n0                 None.  \n1                 None.  \n2                   ___  \n3                   ___  \n4  Chest radiograph ___  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>dicom_id</th>\n      <th>study_id</th>\n      <th>subject_id</th>\n      <th>split</th>\n      <th>impression</th>\n      <th>findings</th>\n      <th>last_paragraph</th>\n      <th>comparison</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>02aa804e-bde0afdd-112c0b34-7bc16630-4e384014</td>\n      <td>50414267</td>\n      <td>10000032</td>\n      <td>train</td>\n      <td>There is no focal consolidation, pleural effus...</td>\n      <td>There is no focal consolidation, pleural effus...</td>\n      <td>NaN</td>\n      <td>None.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962</td>\n      <td>50414267</td>\n      <td>10000032</td>\n      <td>train</td>\n      <td>There is no focal consolidation, pleural effus...</td>\n      <td>There is no focal consolidation, pleural effus...</td>\n      <td>NaN</td>\n      <td>None.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab</td>\n      <td>53189527</td>\n      <td>10000032</td>\n      <td>train</td>\n      <td>The cardiac, mediastinal and hilar contours ar...</td>\n      <td>The cardiac, mediastinal and hilar contours ar...</td>\n      <td>NaN</td>\n      <td>___</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>e084de3b-be89b11e-20fe3f9f-9c8d8dfe-4cfd202c</td>\n      <td>53189527</td>\n      <td>10000032</td>\n      <td>train</td>\n      <td>The cardiac, mediastinal and hilar contours ar...</td>\n      <td>The cardiac, mediastinal and hilar contours ar...</td>\n      <td>NaN</td>\n      <td>___</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714</td>\n      <td>53911762</td>\n      <td>10000032</td>\n      <td>train</td>\n      <td>Single frontal view of the chest provided. The...</td>\n      <td>Single frontal view of the chest provided. The...</td>\n      <td>NaN</td>\n      <td>Chest radiograph ___</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = pd.read_csv(splits_path)\n",
    "reports = pd.read_csv(reports_path)\n",
    "reports.findings = reports.findings.replace(r'\\n', ' ', regex=True)\n",
    "reports.impression = reports.findings.replace(r'\\n', ' ', regex=True)\n",
    "reports.findings = reports.findings.replace(r'\\t', ' ', regex=True)\n",
    "reports.impression = reports.findings.replace(r'\\t', ' ', regex=True)\n",
    "reports.findings = reports.findings.replace(r'\\s{2,}', ' ', regex=True)\n",
    "reports.impression = reports.findings.replace(r'\\s{2,}', ' ', regex=True)\n",
    "reports.rename(columns={'study': 'study_id'}, inplace=True)\n",
    "reports.study_id = reports.study_id.str[1:].astype('int32')\n",
    "df = pd.merge(splits, reports, on='study_id')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the findings and impression sections from the training set:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "304346"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports = df.loc[df.split == 'train'].drop_duplicates(subset=['study_id']).findings.dropna().tolist()\n",
    "reports += df.loc[df.split == 'train'].drop_duplicates(subset=['study_id']).impression.dropna().tolist()\n",
    "len(reports)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Constructing a byte-pair BPE tokenizer based on https://huggingface.co/course/chapter6/8?fw=pt#building-a-bpe-tokenizer-from-scratch:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Byte level pre-tokenizer:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "[('A', (0, 1)),\n ('Ġsingle', (1, 8)),\n ('Ġportable', (8, 17)),\n ('Ġsemi', (17, 22)),\n ('-', (22, 23)),\n ('erect', (23, 28)),\n ('Ġchest', (28, 34)),\n ('Ġradiograph', (34, 45)),\n ('Ġwas', (45, 49)),\n ('Ġobtained', (49, 58)),\n ('.', (58, 59)),\n ('ĠPulmonary', (59, 69)),\n ('Ġaeration', (69, 78)),\n ('Ġhas', (78, 82)),\n ('Ġdecreased', (82, 92)),\n ('.', (92, 93)),\n ('ĠModerate', (93, 102)),\n ('Ġto', (102, 105)),\n ('Ġlarge', (105, 111)),\n ('Ġlayering', (111, 120)),\n ('Ġright', (120, 126)),\n ('Ġpleural', (126, 134)),\n ('Ġeffusion', (134, 143)),\n ('Ġhas', (143, 147)),\n ('Ġincreased', (147, 157)),\n ('.', (157, 158)),\n ('ĠLoculated', (158, 168)),\n ('Ġintra', (168, 174)),\n ('-', (174, 175)),\n ('abdominal', (175, 184)),\n ('Ġair', (184, 188)),\n ('Ġprojects', (188, 197)),\n ('Ġover', (197, 202)),\n ('Ġthe', (202, 206)),\n ('Ġright', (206, 212)),\n ('Ġlung', (212, 217)),\n ('Ġbase', (217, 222)),\n ('.', (222, 223)),\n ('ĠCentral', (223, 231)),\n ('Ġpulmonary', (231, 241)),\n ('Ġvascular', (241, 250)),\n ('Ġcongestion', (250, 261)),\n ('Ġis', (261, 264)),\n ('Ġsimilar', (264, 272)),\n ('.', (272, 273)),\n ('ĠCardiomegaly', (273, 286)),\n ('Ġis', (286, 289)),\n ('Ġunchanged', (289, 299)),\n ('.', (299, 300)),\n ('ĠAn', (300, 303)),\n ('Ġendotracheal', (303, 316)),\n ('Ġtube', (316, 321)),\n ('Ġends', (321, 326)),\n ('Ġ2', (326, 328)),\n ('.', (328, 329)),\n ('5', (329, 330)),\n ('Ġcm', (330, 333)),\n ('Ġabove', (333, 339)),\n ('Ġthe', (339, 343)),\n ('Ġcarina', (343, 350)),\n ('.', (350, 351)),\n ('ĠAn', (351, 354)),\n ('Ġenteric', (354, 362)),\n ('Ġtube', (362, 367)),\n ('Ġpasses', (367, 374)),\n ('Ġinferiorly', (374, 385)),\n ('Ġbelow', (385, 391)),\n ('Ġthe', (391, 395)),\n ('Ġfilm', (395, 400)),\n ('.', (400, 401)),\n ('ĠA', (401, 403)),\n ('Ġright', (403, 409)),\n ('Ġsubclavian', (409, 420)),\n ('Ġcatheter', (420, 429)),\n ('Ġterminates', (429, 440)),\n ('Ġat', (440, 443)),\n ('Ġthe', (443, 447)),\n ('Ġcavoatrial', (447, 458)),\n ('Ġjunction', (458, 467)),\n ('.', (467, 468))]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(reports[408])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train tokenizer on corpus:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "23084"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = tokenizers.trainers.BpeTrainer(special_tokens=['[UNK]', '[BOS]', '[EOS]', '[SEP]', '[PAD]', '[MASK]'])\n",
    "tokenizer.train_from_iterator(reports, trainer)\n",
    "tokenizer.get_vocab_size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Byte-level decoder:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "tokenizer.decoder = tokenizers.decoders.ByteLevel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wrap using HFs PreTrainedTokenizerFast:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "tokenizer = transformers.PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token='[UNK]',\n",
    "    pad_token='[PAD]',\n",
    "    bos_token='[BOS]',\n",
    "    cls_token='[BOS]',\n",
    "    sep_token='[SEP]',\n",
    "    eos_token='[EOS]',\n",
    "    mask_token='[MASK]',\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save tokenizer:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "('B:/work/Checkpoints/mimic-cxr-tokenizers/bpe_findings_impression\\\\tokenizer_config.json',\n 'B:/work/Checkpoints/mimic-cxr-tokenizers/bpe_findings_impression\\\\special_tokens_map.json',\n 'B:/work/Checkpoints/mimic-cxr-tokenizers/bpe_findings_impression\\\\tokenizer.json')"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "version = 'bpe_findings_impression'\n",
    "save_dir = f'B:/work/Checkpoints/mimic-cxr-tokenizers/{version}'\n",
    "\n",
    "Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "tokenizer.save_pretrained(save_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load tokenizer:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "loaded_tokenizer = transformers.PreTrainedTokenizerFast.from_pretrained(save_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['[BOS]', '[EOS]', '[UNK]', '[SEP]', '[PAD]', '[MASK]']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_tokenizer.all_special_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}